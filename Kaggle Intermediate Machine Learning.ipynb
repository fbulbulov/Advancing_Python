{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Intermediate Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data\n",
    "X_full = pd.read_csv('data/train.csv')\n",
    "X_test_full = pd.read_csv('data/train.csv')\n",
    "\n",
    "# Obtain target and predictors\n",
    "y = X_full.SalePrice\n",
    "features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n",
    "X = X_full[features].copy()\n",
    "X_test = X_test_full[features].copy()\n",
    "\n",
    "# Break off validation set from training data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LotArea</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>11694</td>\n",
       "      <td>2007</td>\n",
       "      <td>1828</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>6600</td>\n",
       "      <td>1962</td>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>13360</td>\n",
       "      <td>1921</td>\n",
       "      <td>964</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>13265</td>\n",
       "      <td>2002</td>\n",
       "      <td>1689</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>13704</td>\n",
       "      <td>2001</td>\n",
       "      <td>1541</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     LotArea  YearBuilt  1stFlrSF  2ndFlrSF  FullBath  BedroomAbvGr  \\\n",
       "618    11694       2007      1828         0         2             3   \n",
       "870     6600       1962       894         0         1             2   \n",
       "92     13360       1921       964         0         1             2   \n",
       "817    13265       2002      1689         0         2             3   \n",
       "302    13704       2001      1541         0         2             3   \n",
       "\n",
       "     TotRmsAbvGrd  \n",
       "618             9  \n",
       "870             5  \n",
       "92              5  \n",
       "817             7  \n",
       "302             6  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Define the models\n",
    "model_1 = RandomForestRegressor(n_estimators=50, random_state=0)\n",
    "model_2 = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "model_3 = RandomForestRegressor(n_estimators=100, criterion='mae', random_state=0)\n",
    "model_4 = RandomForestRegressor(n_estimators=200, min_samples_split=20, random_state=0)\n",
    "model_5 = RandomForestRegressor(n_estimators=100, max_depth=7, random_state=0)\n",
    "\n",
    "models = [model_1, model_2, model_3, model_4, model_5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select the best model out of the five, we define a function score_model() below. This function returns the mean absolute error (MAE) from the validation set. Recall that the best model will obtain the lowest MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 MAE: 24015\n",
      "Model 2 MAE: 23740\n",
      "Model 3 MAE: 23528\n",
      "Model 4 MAE: 23996\n",
      "Model 5 MAE: 23706\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Function for comparing different models\n",
    "def score_model(model, X_t=X_train, X_v=X_valid, y_t=y_train, y_v=y_valid):\n",
    "    model.fit(X_t, y_t)\n",
    "    preds = model.predict(X_v)\n",
    "    return mean_absolute_error(y_v, preds)\n",
    "\n",
    "for i in range(0, len(models)):\n",
    "    mae = score_model(models[i])\n",
    "    print(\"Model %d MAE: %d\" % (i+1, mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Generate test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(random_state=1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Define a model\n",
    "my_model = RandomForestRegressor(random_state=1) # Your code here\n",
    "my_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next code cell without changes. The code fits the model to the training and validation data, and then generates test predictions that are saved to a CSV file. These test predictions can be submitted directly to the competition!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data\n",
    "my_model.fit(X, y)\n",
    "\n",
    "# Generate test predictions\n",
    "preds_test = my_model.predict(X_test)\n",
    "\n",
    "# Save predictions in format used for competition scoring\n",
    "output = pd.DataFrame({'Id': X_test.index,\n",
    "                       'SalePrice': preds_test})\n",
    "output.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three Approaches\n",
    "1) A Simple Option: Drop Columns with Missing Values\n",
    "\n",
    "2) A Better Option: Imputation: fills in the missing values with some number. For instance, we can fill in the mean value along each column.\n",
    "\n",
    "3) An Extension To Imputation: In this approach, we impute the missing values, as before. And, additionally, for each column with missing entries in the original dataset, we add a new column that shows the location of the imputed entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "data=pd.read_csv('data/melb_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the example, we will work with the Melbourne Housing dataset. \n",
    "#Our model will use information such as the number of rooms and land size to predict home price.\n",
    "\n",
    "# Select target\n",
    "y = data.Price\n",
    "\n",
    "# To keep things simple, we'll use only numerical predictors\n",
    "melb_predictors = data.drop(['Price'], axis=1)\n",
    "X = melb_predictors.select_dtypes(exclude=['object'])#dropping object types\n",
    "\n",
    "# Divide data into training and validation subsets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Function to Measure Quality of Each Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Function for comparing different approaches\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=10, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Score from Approach 1 (Drop Columns with Missing Values)** : 183550.22\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 1 (Drop columns with missing values):\n",
      "183550.22137772635\n"
     ]
    }
   ],
   "source": [
    "# Get names of columns with missing values\n",
    "cols_with_missing = [col for col in X_train.columns\n",
    "                     if X_train[col].isnull().any()]\n",
    "\n",
    "# Drop columns in training and validation data\n",
    "reduced_X_train = X_train.drop(cols_with_missing, axis=1)\n",
    "reduced_X_valid = X_valid.drop(cols_with_missing, axis=1)\n",
    "\n",
    "print(\"MAE from Approach 1 (Drop columns with missing values):\")\n",
    "print(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Score from Approach 2 (Imputation):** 178166.46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 2 (Imputation):\n",
      "178166.46269899711\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
    "imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_valid.columns\n",
    "\n",
    "print(\"MAE from Approach 2 (Imputation):\")\n",
    "print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Score from Approach 3 (An Extension to Imputation):** 178927.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 3 (An Extension to Imputation):\n",
      "178927.503183954\n"
     ]
    }
   ],
   "source": [
    "# Make copy to avoid changing original data (when imputing)\n",
    "X_train_plus = X_train.copy()\n",
    "X_valid_plus = X_valid.copy()\n",
    "\n",
    "# Make new columns indicating what will be imputed\n",
    "for col in cols_with_missing:\n",
    "    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n",
    "    X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()\n",
    "\n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))\n",
    "imputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train_plus.columns = X_train_plus.columns\n",
    "imputed_X_valid_plus.columns = X_valid_plus.columns\n",
    "\n",
    "print(\"MAE from Approach 3 (An Extension to Imputation):\")\n",
    "print(score_dataset(imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, why did imputation perform better than dropping the columns?\n",
    "The training data has 10864 rows and 12 columns, where three columns contain missing data. For each column, less than half of the entries are missing. Thus, dropping the columns removes a lot of useful information, and so it makes sense that imputation would perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10864, 12)\n",
      "Car               49\n",
      "BuildingArea    5156\n",
      "YearBuilt       4307\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Shape of training data (num_rows, num_columns)\n",
    "print(X_train.shape)\n",
    "\n",
    "# Number of missing values in each column of training data\n",
    "missing_val_count_by_column = (X_train.isnull().sum())\n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data\n",
    "X_full = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing target, separate target from predictors\n",
    "X_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = X_full.SalePrice\n",
    "X_full.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "# To keep things simple, we'll use only numerical predictors\n",
    "X = X_full.select_dtypes(exclude=['object'])\n",
    "X_test = X_test_full.select_dtypes(exclude=['object'])\n",
    "\n",
    "# Break off validation set from training data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>...</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>619</td>\n",
       "      <td>20</td>\n",
       "      <td>90.0</td>\n",
       "      <td>11694</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>2007</td>\n",
       "      <td>452.0</td>\n",
       "      <td>48</td>\n",
       "      <td>...</td>\n",
       "      <td>774</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>260</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>871</td>\n",
       "      <td>20</td>\n",
       "      <td>60.0</td>\n",
       "      <td>6600</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1962</td>\n",
       "      <td>1962</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>308</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>93</td>\n",
       "      <td>30</td>\n",
       "      <td>80.0</td>\n",
       "      <td>13360</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1921</td>\n",
       "      <td>2006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>713</td>\n",
       "      <td>...</td>\n",
       "      <td>432</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>818</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13265</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2002</td>\n",
       "      <td>2002</td>\n",
       "      <td>148.0</td>\n",
       "      <td>1218</td>\n",
       "      <td>...</td>\n",
       "      <td>857</td>\n",
       "      <td>150</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>303</td>\n",
       "      <td>20</td>\n",
       "      <td>118.0</td>\n",
       "      <td>13704</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>843</td>\n",
       "      <td>468</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id  MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  \\\n",
       "618  619          20         90.0    11694            9            5   \n",
       "870  871          20         60.0     6600            5            5   \n",
       "92    93          30         80.0    13360            5            7   \n",
       "817  818          20          NaN    13265            8            5   \n",
       "302  303          20        118.0    13704            7            5   \n",
       "\n",
       "     YearBuilt  YearRemodAdd  MasVnrArea  BsmtFinSF1  ...  GarageArea  \\\n",
       "618       2007          2007       452.0          48  ...         774   \n",
       "870       1962          1962         0.0           0  ...         308   \n",
       "92        1921          2006         0.0         713  ...         432   \n",
       "817       2002          2002       148.0        1218  ...         857   \n",
       "302       2001          2002       150.0           0  ...         843   \n",
       "\n",
       "     WoodDeckSF  OpenPorchSF  EnclosedPorch  3SsnPorch  ScreenPorch  PoolArea  \\\n",
       "618           0          108              0          0          260         0   \n",
       "870           0            0              0          0            0         0   \n",
       "92            0            0             44          0            0         0   \n",
       "817         150           59              0          0            0         0   \n",
       "302         468           81              0          0            0         0   \n",
       "\n",
       "     MiscVal  MoSold  YrSold  \n",
       "618        0       7    2007  \n",
       "870        0       8    2009  \n",
       "92         0       8    2009  \n",
       "817        0       7    2008  \n",
       "302        0       1    2006  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part A\n",
    "\n",
    "Step 1: Preliminary investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1168, 37)\n",
      "LotFrontage    212\n",
      "MasVnrArea       6\n",
      "GarageYrBlt     58\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Shape of training data (num_rows, num_columns)\n",
    "print(X_train.shape)\n",
    "\n",
    "# Number of missing values in each column of training data\n",
    "missing_val_count_by_column = (X_train.isnull().sum())\n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the line below: How many rows are in the training data?\n",
    "num_rows = 1168\n",
    "\n",
    "# Fill in the line below: How many columns in the training data\n",
    "# have missing values?\n",
    "num_cols_with_missing = 3\n",
    "\n",
    "# Fill in the line below: How many missing entries are contained in \n",
    "# all of the training data?\n",
    "tot_missing = 212 + 6 + 58"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part B\n",
    "\n",
    "Considering your answers above, what do you think is likely the best approach to dealing with the missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Function for comparing different approaches\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Drop columns with missing values\n",
    "\n",
    "\n",
    "In this step, you'll preprocess the data in `X_train` and `X_valid` to remove columns with missing values.  Set the preprocessed DataFrames to `reduced_X_train` and `reduced_X_valid`, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the line below: get names of columns with missing values\n",
    "# Your code here\n",
    "\n",
    "# Fill in the lines below: drop columns in training and validation data\n",
    "cols_with_missing = [col for col in X_train.columns\n",
    "                    if X_train[col].isnull().any()]\n",
    "\n",
    "\n",
    "reduced_X_train = X_train.drop(cols_with_missing, axis=1)\n",
    "reduced_X_valid = X_valid.drop(cols_with_missing, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LotFrontage    212\n",
      "MasVnrArea       6\n",
      "GarageYrBlt     58\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_val_count_by_column1 = (reduced_X_train.isnull().sum())\n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0])\n",
    "\n",
    "#reduced_X_train\n",
    "reduced_X_valid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (Drop columns with missing values):\n",
      "17952.59\n"
     ]
    }
   ],
   "source": [
    "print(\"MAE (Drop columns with missing values):\")\n",
    "print(round(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Imputation\n",
    "\n",
    "Part A\n",
    "\n",
    "Use the next code cell to impute missing values with the mean value along each column.  Set the preprocessed DataFrames to `imputed_X_train` and `imputed_X_valid`.  Make sure that the column names match those in `X_train` and `X_valid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "\n",
    "# Fill in the lines below: imputation\n",
    " # Your code here\n",
    "imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
    "imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n",
    "\n",
    "# Fill in the lines below: imputation removed column names; put them back\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_valid.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (Drop columns with missing values):\n",
      "18250.61\n"
     ]
    }
   ],
   "source": [
    "print(\"MAE (Drop columns with missing values):\")\n",
    "print(round(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Generate test predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final step, you'll use any approach of your choosing to deal with missing values. Once you've preprocessed the training and validation features, you'll train and evaluate a random forest model. Then, you'll preprocess the test data before generating predictions that can be submitted to the competition!\n",
    "\n",
    "Part A\n",
    "Use the next code cell to preprocess the training and validation data. Set the preprocessed DataFrames to final_X_train and final_X_valid. You can use any approach of your choosing here! in order for this step to be marked as correct, you need only ensure:\n",
    "\n",
    "the preprocessed DataFrames have the same number of columns,\n",
    "the preprocessed DataFrames have no missing values,\n",
    "final_X_train and y_train have the same number of rows, and\n",
    "final_X_valid and y_valid have the same number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_imputer=SimpleImputer(strategy='median')\n",
    "final_X_train=pd.DataFrame(my_imputer.fit_transform(X_train))\n",
    "final_X_valid=pd.DataFrame(my_imputer.transform(X_valid))\n",
    "\n",
    "final_X_train.columns=X_train.columns\n",
    "final_X_valid.columns=X_valid.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next code cell to train and evaluate a random forest model. (Note that we don't use the score_dataset() function above, because we will soon use the trained model to generate test predictions!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (Your approach):\n",
      "18103.602945205483\n"
     ]
    }
   ],
   "source": [
    "# Define and fit model\n",
    "model=RandomForestRegressor(n_estimators=100,random_state=0)\n",
    "model.fit(final_X_train, y_train)\n",
    "\n",
    "# Get validation predictions and MAE\n",
    "\n",
    "preds_valid=model.predict(final_X_valid)\n",
    "print(\"MAE (Your approach):\")\n",
    "print(mean_absolute_error(y_valid, preds_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part B\n",
    "Use the next code cell to preprocess your test data. Make sure that you use a method that agrees with how you preprocessed the training and validation data, and set the preprocessed test features to final_X_test.\n",
    "\n",
    "Then, use the preprocessed test features and the trained model to generate test predictions in preds_test.\n",
    "\n",
    "In order for this step to be marked correct, you need only ensure:\n",
    "\n",
    "the preprocessed test DataFrame has no missing values, and\n",
    "final_X_test has the same number of rows as X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the line below: preprocess test data\n",
    "#final_X_test = pd.DataFrame(my_imputer.transform(X_test))\n",
    "\n",
    "# Fill in the line below: get test predictions\n",
    "#preds_test = pd.DataFrame(final_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three Approaches\n",
    "1) Drop Categorical Variables\n",
    "The easiest approach to dealing with categorical variables is to simply remove them from the dataset. This approach will only work well if the columns did not contain useful information.\n",
    "\n",
    "2) Label Encoding\n",
    "Label encoding assigns each unique value to a different integer.\n",
    "\n",
    "3) One-Hot Encoding\n",
    "One-hot encoding creates new columns indicating the presence (or absence) of each possible value in the original data. To understand this, we'll work through an example.\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py:4163: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Method</th>\n",
       "      <th>Regionname</th>\n",
       "      <th>Rooms</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Postcode</th>\n",
       "      <th>Bedroom2</th>\n",
       "      <th>Bathroom</th>\n",
       "      <th>Landsize</th>\n",
       "      <th>Lattitude</th>\n",
       "      <th>Longtitude</th>\n",
       "      <th>Propertycount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12167</th>\n",
       "      <td>u</td>\n",
       "      <td>S</td>\n",
       "      <td>Southern Metropolitan</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3182.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-37.85984</td>\n",
       "      <td>144.9867</td>\n",
       "      <td>13240.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6524</th>\n",
       "      <td>h</td>\n",
       "      <td>SA</td>\n",
       "      <td>Western Metropolitan</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3016.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>-37.85800</td>\n",
       "      <td>144.9005</td>\n",
       "      <td>6380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8413</th>\n",
       "      <td>h</td>\n",
       "      <td>S</td>\n",
       "      <td>Western Metropolitan</td>\n",
       "      <td>3</td>\n",
       "      <td>12.6</td>\n",
       "      <td>3020.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>555.0</td>\n",
       "      <td>-37.79880</td>\n",
       "      <td>144.8220</td>\n",
       "      <td>3755.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2919</th>\n",
       "      <td>u</td>\n",
       "      <td>SP</td>\n",
       "      <td>Northern Metropolitan</td>\n",
       "      <td>3</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3046.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>-37.70830</td>\n",
       "      <td>144.9158</td>\n",
       "      <td>8870.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6043</th>\n",
       "      <td>h</td>\n",
       "      <td>S</td>\n",
       "      <td>Western Metropolitan</td>\n",
       "      <td>3</td>\n",
       "      <td>13.3</td>\n",
       "      <td>3020.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>673.0</td>\n",
       "      <td>-37.76230</td>\n",
       "      <td>144.8272</td>\n",
       "      <td>4217.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Type Method             Regionname  Rooms  Distance  Postcode  Bedroom2  \\\n",
       "12167    u      S  Southern Metropolitan      1       5.0    3182.0       1.0   \n",
       "6524     h     SA   Western Metropolitan      2       8.0    3016.0       2.0   \n",
       "8413     h      S   Western Metropolitan      3      12.6    3020.0       3.0   \n",
       "2919     u     SP  Northern Metropolitan      3      13.0    3046.0       3.0   \n",
       "6043     h      S   Western Metropolitan      3      13.3    3020.0       3.0   \n",
       "\n",
       "       Bathroom  Landsize  Lattitude  Longtitude  Propertycount  \n",
       "12167       1.0       0.0  -37.85984    144.9867        13240.0  \n",
       "6524        2.0     193.0  -37.85800    144.9005         6380.0  \n",
       "8413        1.0     555.0  -37.79880    144.8220         3755.0  \n",
       "2919        1.0     265.0  -37.70830    144.9158         8870.0  \n",
       "6043        1.0     673.0  -37.76230    144.8272         4217.0  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data\n",
    "data=pd.read_csv('data/melb_data.csv')\n",
    "\n",
    "# Separate target from predictors\n",
    "y = data.Price\n",
    "X = data.drop(['Price'], axis=1)\n",
    "\n",
    "# Divide data into training and validation subsets\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "# Drop columns with missing values (simplest approach)\n",
    "cols_with_missing = [col for col in X_train_full.columns if X_train_full[col].isnull().any()] \n",
    "X_train_full.drop(cols_with_missing, axis=1, inplace=True)\n",
    "X_valid_full.drop(cols_with_missing, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# \"Cardinality\" means the number of unique values in a column\n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "low_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n",
    "                        X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = low_cardinality_cols + numerical_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we obtain a list of all of the categorical variables in the training data.\n",
    "\n",
    "We do this by checking the data type (or dtype) of each column. The object dtype indicates a column has text (there are other things it could theoretically be, but that's unimportant for our purposes). For this dataset, the columns with text indicate categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical variables:\n",
      "['Type', 'Method', 'Regionname']\n"
     ]
    }
   ],
   "source": [
    "s = (X_train.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "print(\"Categorical variables:\")\n",
    "print(object_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Function to Measure Quality of Each Approach¶\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Function for comparing different approaches\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Score from Approach 1 (Drop Categorical Variables)**: 175703.48\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 1 (Drop categorical variables):\n",
      "175703.48185157913\n"
     ]
    }
   ],
   "source": [
    "drop_X_train = X_train.select_dtypes(exclude=['object'])\n",
    "drop_X_valid = X_valid.select_dtypes(exclude=['object'])\n",
    "\n",
    "print(\"MAE from Approach 1 (Drop categorical variables):\")\n",
    "print(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Score from Approach 2 (Label Encoding)**: 165936.405\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn has a LabelEncoder class that can be used to get label encodings. We loop over the categorical variables and apply the label encoder separately to each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 2 (Label Encoding):\n",
      "165936.40548390493\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Make copy to avoid changing original data \n",
    "label_X_train = X_train.copy()\n",
    "label_X_valid = X_valid.copy()\n",
    "\n",
    "# Apply label encoder to each column with categorical data\n",
    "label_encoder = LabelEncoder()\n",
    "for col in object_cols:\n",
    "    label_X_train[col] = label_encoder.fit_transform(X_train[col])\n",
    "    label_X_valid[col] = label_encoder.transform(X_valid[col])\n",
    "\n",
    "print(\"MAE from Approach 2 (Label Encoding):\") \n",
    "print(score_dataset(label_X_train, label_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell above, for each column, we randomly assign each unique value to a different integer. This is a common approach that is simpler than providing custom labels; however, we can expect an additional boost in performance if we provide better-informed labels for all ordinal variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Score from Approach 3 (One-Hot Encoding)** :166089.49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 3 (One-Hot Encoding):\n",
      "166089.4893009678\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Apply one-hot encoder to each column with categorical data\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_valid.index = X_valid.index\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "num_X_train = X_train.drop(object_cols, axis=1)\n",
    "num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
    "\n",
    "print(\"MAE from Approach 3 (One-Hot Encoding):\") \n",
    "print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Categorical Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "X = pd.read_csv('data/train.csv')\n",
    "X_test = pd.read_csv('data/train.csv', index_col='Id')\n",
    "\n",
    "# Remove rows with missing target, separate target from predictors\n",
    "X.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = X.SalePrice\n",
    "X.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "# To keep things simple, we'll drop columns with missing values\n",
    "cols_with_missing = [col for col in X.columns if X[col].isnull().any()] \n",
    "X.drop(cols_with_missing, axis=1, inplace=True)\n",
    "X_test.drop(cols_with_missing, axis=1, inplace=True)\n",
    "\n",
    "# Break off validation set from training data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y,\n",
    "                                                      train_size=0.8, test_size=0.2,\n",
    "                                                      random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>...</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>619</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>11694</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>260</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2007</td>\n",
       "      <td>New</td>\n",
       "      <td>Partial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>871</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>6600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>93</td>\n",
       "      <td>30</td>\n",
       "      <td>RL</td>\n",
       "      <td>13360</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>HLS</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>818</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>13265</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>CulDSac</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>303</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>13704</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id  MSSubClass MSZoning  LotArea Street LotShape LandContour Utilities  \\\n",
       "618  619          20       RL    11694   Pave      Reg         Lvl    AllPub   \n",
       "870  871          20       RL     6600   Pave      Reg         Lvl    AllPub   \n",
       "92    93          30       RL    13360   Pave      IR1         HLS    AllPub   \n",
       "817  818          20       RL    13265   Pave      IR1         Lvl    AllPub   \n",
       "302  303          20       RL    13704   Pave      IR1         Lvl    AllPub   \n",
       "\n",
       "    LotConfig LandSlope  ... OpenPorchSF EnclosedPorch 3SsnPorch ScreenPorch  \\\n",
       "618    Inside       Gtl  ...         108             0         0         260   \n",
       "870    Inside       Gtl  ...           0             0         0           0   \n",
       "92     Inside       Gtl  ...           0            44         0           0   \n",
       "817   CulDSac       Gtl  ...          59             0         0           0   \n",
       "302    Corner       Gtl  ...          81             0         0           0   \n",
       "\n",
       "    PoolArea  MiscVal  MoSold  YrSold  SaleType SaleCondition  \n",
       "618        0        0       7    2007       New       Partial  \n",
       "870        0        0       8    2009        WD        Normal  \n",
       "92         0        0       8    2009        WD        Normal  \n",
       "817        0        0       7    2008        WD        Normal  \n",
       "302        0        0       1    2006        WD        Normal  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# function for comparing different approaches\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Drop columns with categorical data : 17952.59\n",
    "\n",
    "You'll get started with the most straightforward approach.  Use the code cell below to preprocess the data in `X_train` and `X_valid` to remove columns with categorical data.  Set the preprocessed DataFrames to `drop_X_train` and `drop_X_valid`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 1 (Drop categorical variables):\n",
      "17952.591404109586\n"
     ]
    }
   ],
   "source": [
    "drop_X_train = drop_X_train = X_train.select_dtypes(exclude=['object'])\n",
    "drop_X_valid = drop_X_valid = X_valid.select_dtypes(exclude=['object'])\n",
    "\n",
    "print(\"MAE from Approach 1 (Drop categorical variables):\")\n",
    "print(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'Condition2' column in training data: ['Norm' 'PosA' 'Feedr' 'PosN' 'Artery' 'RRAe']\n",
      "\n",
      "Unique values in 'Condition2' column in validation data: ['Norm' 'RRAn' 'RRNn' 'Artery' 'Feedr' 'PosN']\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique values in 'Condition2' column in training data:\", X_train['Condition2'].unique())\n",
    "print(\"\\nUnique values in 'Condition2' column in validation data:\", X_valid['Condition2'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Label encoding: 17675.94\n",
    "\n",
    "Part A\n",
    "\n",
    "If you now write code to: \n",
    "- fit a label encoder to the training data, and then \n",
    "- use it to transform both the training and validation data, \n",
    "\n",
    "you'll get an error.  Can you see why this is the case?  (_You'll need  to use the above output to answer this question._)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns that will be label encoded: ['MSZoning', 'Street', 'LotShape', 'LandContour', 'LotConfig', 'BldgType', 'HouseStyle', 'ExterQual', 'CentralAir', 'KitchenQual', 'PavedDrive', 'SaleCondition']\n",
      "\n",
      "Categorical columns that will be dropped from the dataset: ['Functional', 'Exterior2nd', 'Neighborhood', 'RoofMatl', 'RoofStyle', 'HeatingQC', 'Heating', 'LandSlope', 'Condition1', 'SaleType', 'Foundation', 'Exterior1st', 'Utilities', 'Condition2', 'ExterCond']\n"
     ]
    }
   ],
   "source": [
    "# All categorical columns\n",
    "object_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n",
    "\n",
    "# Columns that can be safely label encoded\n",
    "good_label_cols = [col for col in object_cols if \n",
    "                   set(X_train[col]) == set(X_valid[col])]\n",
    "        \n",
    "# Problematic columns that will be dropped from the dataset\n",
    "bad_label_cols = list(set(object_cols)-set(good_label_cols))\n",
    "        \n",
    "print('Categorical columns that will be label encoded:', good_label_cols)\n",
    "print('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part B\n",
    "\n",
    "Use the next code cell to label encode the data in X_train and X_valid. Set the preprocessed DataFrames to label_X_train and label_X_valid, respectively.\n",
    "\n",
    "We have provided code below to drop the categorical columns in bad_label_cols from the dataset.\n",
    "You should label encode the categorical columns in good_label_cols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 2 (Label Encoding):\n",
      "17675.942500000005\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Drop categorical columns that will not be encoded\n",
    "label_X_train = X_train.drop(bad_label_cols, axis=1)\n",
    "label_X_valid = X_valid.drop(bad_label_cols, axis=1)\n",
    "\n",
    "# Apply label encoder \n",
    "label_encoder = LabelEncoder()\n",
    "for col in set(good_label_cols):\n",
    "    label_X_train[col] = label_encoder.fit_transform(X_train[col])\n",
    "    label_X_valid[col] = label_encoder.transform(X_valid[col])  \n",
    "    \n",
    "print(\"MAE from Approach 2 (Label Encoding):\") \n",
    "print(score_dataset(label_X_train, label_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Street', 2),\n",
       " ('Utilities', 2),\n",
       " ('CentralAir', 2),\n",
       " ('LandSlope', 3),\n",
       " ('PavedDrive', 3),\n",
       " ('LotShape', 4),\n",
       " ('LandContour', 4),\n",
       " ('ExterQual', 4),\n",
       " ('KitchenQual', 4),\n",
       " ('MSZoning', 5),\n",
       " ('LotConfig', 5),\n",
       " ('BldgType', 5),\n",
       " ('ExterCond', 5),\n",
       " ('HeatingQC', 5),\n",
       " ('Condition2', 6),\n",
       " ('RoofStyle', 6),\n",
       " ('Foundation', 6),\n",
       " ('Heating', 6),\n",
       " ('Functional', 6),\n",
       " ('SaleCondition', 6),\n",
       " ('RoofMatl', 7),\n",
       " ('HouseStyle', 8),\n",
       " ('Condition1', 9),\n",
       " ('SaleType', 9),\n",
       " ('Exterior1st', 15),\n",
       " ('Exterior2nd', 16),\n",
       " ('Neighborhood', 25)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get number of unique entries in each column with categorical data\n",
    "object_nunique = list(map(lambda col: X_train[col].nunique(), object_cols))\n",
    "d = dict(zip(object_cols, object_nunique))\n",
    "\n",
    "# Print number of unique entries by column, in ascending order\n",
    "sorted(d.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Investigating cardinality\n",
    "\n",
    "Part A\n",
    "\n",
    "The output above shows, for each column with categorical data, the number of unique values in the column.  For instance, the `'Street'` column in the training data has two unique values: `'Grvl'` and `'Pave'`, corresponding to a gravel road and a paved road, respectively.\n",
    "\n",
    "We refer to the number of unique entries of a categorical variable as the **cardinality** of that categorical variable.  For instance, the `'Street'` variable has cardinality 2.\n",
    "\n",
    "Use the output above to answer the questions below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the line below: How many categorical variables in the training data\n",
    "# have cardinality greater than 10?\n",
    "high_cardinality_numcols = 3\n",
    "\n",
    "# Fill in the line below: How many columns are needed to one-hot encode the \n",
    "# 'Neighborhood' variable in the training data?\n",
    "num_cols_neighborhood = 25\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part B\n",
    "\n",
    "One Hot Coding: \n",
    "\n",
    "For large datasets with many rows, one-hot encoding can greatly expand the size of the dataset. For this reason, we typically will only one-hot encode columns with relatively low cardinality. Then, high cardinality columns can either be dropped from the dataset, or we can use label encoding.\n",
    "\n",
    "As an example, consider a dataset with 10,000 rows, and containing one categorical column with 100 unique entries.\n",
    "\n",
    "If this column is replaced with the corresponding one-hot encoding, how many entries are added to the dataset?\n",
    "If we instead replace the column with the label encoding, how many entries are added?\n",
    "Use your answers to fill in the lines below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the line below: How many entries are added to the dataset by \n",
    "# replacing the column with a one-hot encoding?\n",
    "OH_entries_added = 1e4*100 - 1e4\n",
    "\n",
    "# Fill in the line below: How many entries are added to the dataset by\n",
    "# replacing the column with a label encoding?\n",
    "label_entries_added = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, you'll experiment with one-hot encoding. But, instead of encoding all of the categorical variables in the dataset, you'll only create a one-hot encoding for columns with cardinality less than 10.\n",
    "\n",
    "Run the code cell below without changes to set low_cardinality_cols to a Python list containing the columns that will be one-hot encoded. Likewise, high_cardinality_cols contains a list of categorical columns that will be dropped from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns that will be one-hot encoded: ['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'ExterQual', 'ExterCond', 'Foundation', 'Heating', 'HeatingQC', 'CentralAir', 'KitchenQual', 'Functional', 'PavedDrive', 'SaleType', 'SaleCondition']\n",
      "\n",
      "Categorical columns that will be dropped from the dataset: ['Exterior1st', 'Neighborhood', 'Exterior2nd']\n"
     ]
    }
   ],
   "source": [
    "# Columns that will be one-hot encoded\n",
    "low_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]\n",
    "\n",
    "# Columns that will be dropped from the dataset\n",
    "high_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n",
    "\n",
    "print('Categorical columns that will be one-hot encoded:', low_cardinality_cols)\n",
    "print('\\nCategorical columns that will be dropped from the dataset:', high_cardinality_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: One-hot encoding: 17514.22\n",
    "\n",
    "Use the next code cell to one-hot encode the data in X_train and X_valid. Set the preprocessed DataFrames to OH_X_train and OH_X_valid, respectively.\n",
    "\n",
    "The full list of categorical columns in the dataset can be found in the Python list object_cols.\n",
    "You should only one-hot encode the categorical columns in low_cardinality_cols. All other categorical columns should be dropped from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 3 (One-Hot Encoding):\n",
      "17514.224246575344\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Use as many lines of code as you need!\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_cardinality_cols]))\n",
    "\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "\n",
    "OH_cols_train.index=X_train.index\n",
    "OH_cols_valid.index=X_valid.index\n",
    "\n",
    "\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "\n",
    "num_X_train=X_train.drop(object_cols,axis=1)\n",
    "num_X_valid=X_valid.drop(object_cols,axis=1)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "OH_X_train=pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_valid=pd.concat([num_X_valid,OH_cols_valid],axis=1)\n",
    "\n",
    "#Results\n",
    "print(\"MAE from Approach 3 (One-Hot Encoding):\") \n",
    "print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many data scientists hack together models without pipelines, but pipelines have some important benefits. Those include:\n",
    "\n",
    "Cleaner Code: Accounting for data at each step of preprocessing can get messy. With a pipeline, you won't need to manually keep track of your training and validation data at each step.\n",
    "Fewer Bugs: There are fewer opportunities to misapply a step or forget a preprocessing step.\n",
    "Easier to Productionize: It can be surprisingly hard to transition a model from a prototype to something deployable at scale. We won't go into the many related concerns here, but pipelines can help.\n",
    "More Options for Model Validation: You will see an example in the next tutorial, which covers cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Method</th>\n",
       "      <th>Regionname</th>\n",
       "      <th>Rooms</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Postcode</th>\n",
       "      <th>Bedroom2</th>\n",
       "      <th>Bathroom</th>\n",
       "      <th>Car</th>\n",
       "      <th>Landsize</th>\n",
       "      <th>BuildingArea</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>Lattitude</th>\n",
       "      <th>Longtitude</th>\n",
       "      <th>Propertycount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12167</th>\n",
       "      <td>u</td>\n",
       "      <td>S</td>\n",
       "      <td>Southern Metropolitan</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3182.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>-37.85984</td>\n",
       "      <td>144.9867</td>\n",
       "      <td>13240.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6524</th>\n",
       "      <td>h</td>\n",
       "      <td>SA</td>\n",
       "      <td>Western Metropolitan</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3016.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-37.85800</td>\n",
       "      <td>144.9005</td>\n",
       "      <td>6380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8413</th>\n",
       "      <td>h</td>\n",
       "      <td>S</td>\n",
       "      <td>Western Metropolitan</td>\n",
       "      <td>3</td>\n",
       "      <td>12.6</td>\n",
       "      <td>3020.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>555.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-37.79880</td>\n",
       "      <td>144.8220</td>\n",
       "      <td>3755.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2919</th>\n",
       "      <td>u</td>\n",
       "      <td>SP</td>\n",
       "      <td>Northern Metropolitan</td>\n",
       "      <td>3</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3046.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>-37.70830</td>\n",
       "      <td>144.9158</td>\n",
       "      <td>8870.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6043</th>\n",
       "      <td>h</td>\n",
       "      <td>S</td>\n",
       "      <td>Western Metropolitan</td>\n",
       "      <td>3</td>\n",
       "      <td>13.3</td>\n",
       "      <td>3020.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>673.0</td>\n",
       "      <td>673.0</td>\n",
       "      <td>1970.0</td>\n",
       "      <td>-37.76230</td>\n",
       "      <td>144.8272</td>\n",
       "      <td>4217.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Type Method             Regionname  Rooms  Distance  Postcode  Bedroom2  \\\n",
       "12167    u      S  Southern Metropolitan      1       5.0    3182.0       1.0   \n",
       "6524     h     SA   Western Metropolitan      2       8.0    3016.0       2.0   \n",
       "8413     h      S   Western Metropolitan      3      12.6    3020.0       3.0   \n",
       "2919     u     SP  Northern Metropolitan      3      13.0    3046.0       3.0   \n",
       "6043     h      S   Western Metropolitan      3      13.3    3020.0       3.0   \n",
       "\n",
       "       Bathroom  Car  Landsize  BuildingArea  YearBuilt  Lattitude  \\\n",
       "12167       1.0  1.0       0.0           NaN     1940.0  -37.85984   \n",
       "6524        2.0  1.0     193.0           NaN        NaN  -37.85800   \n",
       "8413        1.0  1.0     555.0           NaN        NaN  -37.79880   \n",
       "2919        1.0  1.0     265.0           NaN     1995.0  -37.70830   \n",
       "6043        1.0  2.0     673.0         673.0     1970.0  -37.76230   \n",
       "\n",
       "       Longtitude  Propertycount  \n",
       "12167    144.9867        13240.0  \n",
       "6524     144.9005         6380.0  \n",
       "8413     144.8220         3755.0  \n",
       "2919     144.9158         8870.0  \n",
       "6043     144.8272         4217.0  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv('data/melb_data.csv')\n",
    "\n",
    "# Separate target from predictors\n",
    "y = data.Price\n",
    "X = data.drop(['Price'], axis=1)\n",
    "\n",
    "# Divide data into training and validation subsets\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                                random_state=0)\n",
    "\n",
    "# \"Cardinality\" means the number of unique values in a column\n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "categorical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n",
    "                        X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = categorical_cols + numerical_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Define Preprocessing Steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to how a pipeline bundles together preprocessing and modeling steps, we use the ColumnTransformer class to bundle together different preprocessing steps. The code below:\n",
    "\n",
    "imputes missing values in numerical data, and\n",
    "imputes missing values and applies a one-hot encoding to categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Define the Model\n",
    "Next, we define a random forest model with the familiar RandomForestRegressor class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Create and Evaluate the Pipeline\n",
    "Finally, we use the Pipeline class to define a pipeline that bundles the preprocessing and modeling steps. There are a few important things to notice:\n",
    "\n",
    "With the pipeline, we preprocess the training data and fit the model in a single line of code. (In contrast, without a pipeline, we have to do imputation, one-hot encoding, and model training in separate steps. This becomes especially messy if we have to deal with both numerical and categorical variables!)\n",
    "With the pipeline, we supply the unprocessed features in X_valid to the predict() command, and the pipeline automatically preprocesses the features before generating predictions. (However, without a pipeline, we have to remember to preprocess the validation data before making predictions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 160679.18917034855\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', model)\n",
    "                             ])\n",
    "\n",
    "# Preprocessing of training data, fit model \n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = my_pipeline.predict(X_valid)\n",
    "\n",
    "# Evaluate the model\n",
    "score = mean_absolute_error(y_valid, preds)\n",
    "print('MAE:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Pipelines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>Condition1</th>\n",
       "      <th>Condition2</th>\n",
       "      <th>...</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>RL</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>...</td>\n",
       "      <td>774</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>260</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>RL</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>PosN</td>\n",
       "      <td>Norm</td>\n",
       "      <td>...</td>\n",
       "      <td>308</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>RL</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Grvl</td>\n",
       "      <td>IR1</td>\n",
       "      <td>HLS</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>...</td>\n",
       "      <td>432</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>RL</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>CulDSac</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>...</td>\n",
       "      <td>857</td>\n",
       "      <td>150</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>RL</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>...</td>\n",
       "      <td>843</td>\n",
       "      <td>468</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    MSZoning Street Alley LotShape LandContour Utilities LotConfig LandSlope  \\\n",
       "Id                                                                             \n",
       "619       RL   Pave   NaN      Reg         Lvl    AllPub    Inside       Gtl   \n",
       "871       RL   Pave   NaN      Reg         Lvl    AllPub    Inside       Gtl   \n",
       "93        RL   Pave  Grvl      IR1         HLS    AllPub    Inside       Gtl   \n",
       "818       RL   Pave   NaN      IR1         Lvl    AllPub   CulDSac       Gtl   \n",
       "303       RL   Pave   NaN      IR1         Lvl    AllPub    Corner       Gtl   \n",
       "\n",
       "    Condition1 Condition2  ... GarageArea WoodDeckSF OpenPorchSF  \\\n",
       "Id                         ...                                     \n",
       "619       Norm       Norm  ...        774          0         108   \n",
       "871       PosN       Norm  ...        308          0           0   \n",
       "93        Norm       Norm  ...        432          0           0   \n",
       "818       Norm       Norm  ...        857        150          59   \n",
       "303       Norm       Norm  ...        843        468          81   \n",
       "\n",
       "    EnclosedPorch 3SsnPorch ScreenPorch PoolArea MiscVal MoSold YrSold  \n",
       "Id                                                                      \n",
       "619             0         0         260        0       0      7   2007  \n",
       "871             0         0           0        0       0      8   2009  \n",
       "93             44         0           0        0       0      8   2009  \n",
       "818             0         0           0        0       0      7   2008  \n",
       "303             0         0           0        0       0      1   2006  \n",
       "\n",
       "[5 rows x 76 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data\n",
    "X_full = pd.read_csv('data/train.csv', index_col='Id')\n",
    "X_test_full = pd.read_csv('data/test.csv', index_col='Id')\n",
    "\n",
    "# Remove rows with missing target, separate target from predictors\n",
    "X_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = X_full.SalePrice\n",
    "X_full.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "# Break off validation set from training data\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y, \n",
    "                                                                train_size=0.8, test_size=0.2,\n",
    "                                                                random_state=0)\n",
    "\n",
    "# \"Cardinality\" means the number of unique values in a column\n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "categorical_cols = [cname for cname in X_train_full.columns if\n",
    "                    X_train_full[cname].nunique() < 10 and \n",
    "                    X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train_full.columns if \n",
    "                X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = categorical_cols + numerical_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()\n",
    "X_test = X_test_full[my_cols].copy()\n",
    "\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code cell uses code from the tutorial to preprocess the data and train a model.  Run this code without changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 17861.780102739725\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Define model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('model', model)\n",
    "                     ])\n",
    "\n",
    "# Preprocessing of training data, fit model \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = clf.predict(X_valid)\n",
    "\n",
    "print('MAE:', mean_absolute_error(y_valid, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for numerical data \n",
    "numerical_transformer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Preprocessing for categorical data \n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer',SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot',OneHotEncoder(handle_unknown='ignore',sparse=False))]) # Your code here 加了parse=False\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data \n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Define model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0) # Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 17553.371061643833\n"
     ]
    }
   ],
   "source": [
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', model)\n",
    "                             ])\n",
    "\n",
    "# Preprocessing of training data, fit model \n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = my_pipeline.predict(X_valid)\n",
    "\n",
    "# Evaluate the model\n",
    "score = mean_absolute_error(y_valid, preds)\n",
    "print('MAE:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Generate test predictions\n",
    "\n",
    "Now, you'll use your trained model to generate predictions with the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[127226.5  154266.5  184979.12 ... 150508.17 110219.08 222463.  ]\n"
     ]
    }
   ],
   "source": [
    "preds_test = my_pipeline.predict(X_test)\n",
    "print(preds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test predictions to file\n",
    "output = pd.DataFrame({'Id': X_test.index,\n",
    "                       'SalePrice': preds_test})\n",
    "output.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In cross-validation, we run our modeling process on different subsets of the data to get multiple measures of model quality.\n",
    "\n",
    "For example, we could begin by dividing the data into 5 pieces, each 20% of the full dataset. In this case, we say that we have broken the data into 5 \"folds\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('data/melb_data.csv')\n",
    "\n",
    "# Select subset of predictors\n",
    "cols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\n",
    "X = data[cols_to_use]\n",
    "\n",
    "# Select target\n",
    "y = data.Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#Pipeline \n",
    "my_pipeline = Pipeline(steps=[('preprocessor', SimpleImputer()),\n",
    "                              ('model', RandomForestRegressor(n_estimators=50,\n",
    "                                                              random_state=0))\n",
    "                             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE scores:\n",
      " [301628.7893587  303164.4782723  287298.331666   236061.84754543\n",
      " 260383.45111427]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Multiply by -1 since sklearn calculates *negative* MAE\n",
    "scores = -1 * cross_val_score(my_pipeline, X, y,\n",
    "                              cv=5,\n",
    "                              scoring='neg_mean_absolute_error')\n",
    "\n",
    "print(\"MAE scores:\\n\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scoring parameter chooses a measure of model quality to report: in this case, we chose negative mean absolute error (MAE). The docs for scikit-learn show a list of options.\n",
    "\n",
    "It is a little surprising that we specify negative MAE. Scikit-learn has a convention where all metrics are defined so a high number is better. Using negatives here allows them to be consistent with that convention, though negative MAE is almost unheard of elsewhere.\n",
    "\n",
    "We typically want a single measure of model quality to compare alternative models. So we take the average across experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MAE score (across experiments):\n",
      "277707.3795913405\n"
     ]
    }
   ],
   "source": [
    "print(\"Average MAE score (across experiments):\")\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Cross-Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>548</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>460</td>\n",
       "      <td>298</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>608</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>642</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>60</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>350.0</td>\n",
       "      <td>655</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>836</td>\n",
       "      <td>192</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "Id                                                                          \n",
       "1           60         65.0     8450            7            5       2003   \n",
       "2           20         80.0     9600            6            8       1976   \n",
       "3           60         68.0    11250            7            5       2001   \n",
       "4           70         60.0     9550            7            5       1915   \n",
       "5           60         84.0    14260            8            5       2000   \n",
       "\n",
       "    YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  GarageArea  \\\n",
       "Id                                                    ...               \n",
       "1           2003       196.0         706           0  ...         548   \n",
       "2           1976         0.0         978           0  ...         460   \n",
       "3           2002       162.0         486           0  ...         608   \n",
       "4           1970         0.0         216           0  ...         642   \n",
       "5           2000       350.0         655           0  ...         836   \n",
       "\n",
       "    WoodDeckSF  OpenPorchSF  EnclosedPorch  3SsnPorch  ScreenPorch  PoolArea  \\\n",
       "Id                                                                             \n",
       "1            0           61              0          0            0         0   \n",
       "2          298            0              0          0            0         0   \n",
       "3            0           42              0          0            0         0   \n",
       "4            0           35            272          0            0         0   \n",
       "5          192           84              0          0            0         0   \n",
       "\n",
       "    MiscVal  MoSold  YrSold  \n",
       "Id                           \n",
       "1         0       2    2008  \n",
       "2         0       5    2007  \n",
       "3         0       9    2008  \n",
       "4         0       2    2006  \n",
       "5         0      12    2008  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Read the data\n",
    "train_data = pd.read_csv('data/train.csv', index_col='Id')\n",
    "test_data = pd.read_csv('data/train.csv', index_col='Id')\n",
    "\n",
    "# Remove rows with missing target, separate target from predictors\n",
    "train_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = train_data.SalePrice              \n",
    "train_data.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "# Select numeric columns only\n",
    "numeric_cols = [cname for cname in train_data.columns if train_data[cname].dtype in ['int64', 'float64']]\n",
    "X = train_data[numeric_cols].copy()\n",
    "X_test = test_data[numeric_cols].copy()\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model and pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "my_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', SimpleImputer()),\n",
    "    ('model', RandomForestRegressor(n_estimators=50, random_state=0))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have also learned how to use pipelines in cross-validation.  The code below uses the [`cross_val_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) function to obtain the mean absolute error (MAE), averaged across five different folds.  Recall we set the number of folds with the `cv` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MAE score: 18276.410356164386\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Multiply by -1 since sklearn calculates *negative* MAE\n",
    "scores = -1 * cross_val_score(my_pipeline, X, y,\n",
    "                              cv=5,\n",
    "                              scoring='neg_mean_absolute_error')\n",
    "\n",
    "print(\"Average MAE score:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Write a useful function\n",
    "In this exercise, you'll use cross-validation to select parameters for a machine learning model.\n",
    "\n",
    "Begin by writing a function get_score() that reports the average (over three cross-validation folds) MAE of a machine learning pipeline that uses:\n",
    "\n",
    "the data in X and y to create folds,\n",
    "SimpleImputer() (with all parameters left as default) to replace missing values, and\n",
    "RandomForestRegressor() (with random_state=0) to fit a random forest model.\n",
    "The n_estimators parameter supplied to get_score() is used when setting the number of trees in the random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(n_estimators):\n",
    "    \"\"\"Return the average MAE over 3 CV folds of random forest model.\n",
    "    \n",
    "    Keyword argument:\n",
    "    n_estimators -- the number of trees in the forest\n",
    "    \"\"\"\n",
    "    # Replace this body with your own code\n",
    "    \n",
    "    my_pipeline=Pipeline(steps=\n",
    "                         [('preprocessor', SimpleImputer()), \n",
    "                          ('model',RandomForestRegressor(n_estimators,random_state=0))])\n",
    "    scores=-1*cross_val_score(my_pipeline,X,y, cv=3, scoring='neg_mean_absolute_error')\n",
    "    return scores.mean()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for i in range(1,9):\n",
    "    results[50*i] = get_score(50*i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(list(results.keys()), list(results.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Find the best parameter value\n",
    "Given the results, which value for n_estimators seems best for the random forest model? Use your answer to set the value of n_estimators_best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "n_estimators_best = min(results, key=results.get) \n",
    "print(n_estimators_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting\n",
    "Gradient boosting is a method that goes through cycles to iteratively add models into an ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Farrukh Bulbulov**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
