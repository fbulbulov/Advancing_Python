{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68a05cb3",
   "metadata": {},
   "source": [
    "## Kaggle Natural Language Processing Course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c73b3a4",
   "metadata": {},
   "source": [
    "In this course about Natural Language Processing (NLP), you will use the leading NLP library (spaCy) to take on some of the most important tasks in working with text.\n",
    "\n",
    "By the end, you will be able to use spaCy for:\n",
    "\n",
    "- Basic text processing and pattern matching\n",
    "- Building machine learning models with text\n",
    "- Representing text with word embeddings that numerically capture the meaning of words and documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff2ad5c",
   "metadata": {},
   "source": [
    "SpaCy is the leading library for NLP, and it has quickly become one of the most popular Python frameworks. Most people find it intuitive, and it has excellent documentation.\n",
    "\n",
    "spaCy relies on models that are language-specific and come in different sizes. You can load a spaCy model with spacy.load.\n",
    "\n",
    "For example, here's how you would load the English language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20aac2d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T12:24:26.818082Z",
     "start_time": "2021-07-20T12:24:14.014738Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3d2494",
   "metadata": {},
   "source": [
    "With the model loaded, you can process text like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c220cc87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T12:24:57.348499Z",
     "start_time": "2021-07-20T12:24:57.217575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tea is healthy and calming, don't you think?\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(\"Tea is healthy and calming, don't you think?\")\n",
    "print (doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d093212c",
   "metadata": {},
   "source": [
    "**Tokenizing**\n",
    "\n",
    "This returns a document object that contains tokens. A token is a unit of text in the document, such as individual words and punctuation. SpaCy splits contractions like \"don't\" into two tokens, \"do\" and \"n't\". You can see the tokens by iterating through the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9defd711",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T12:33:32.932393Z",
     "start_time": "2021-07-20T12:33:32.928395Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tea\n",
      "is\n",
      "healthy\n",
      "and\n",
      "calming\n",
      ",\n",
      "do\n",
      "n't\n",
      "you\n",
      "think\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print (token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8607084",
   "metadata": {},
   "source": [
    "Iterating through a document gives you token objects. Each of these tokens comes with additional information. In most cases, the important ones are `token.lemma_` and `token.is_stop`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc7b25f",
   "metadata": {},
   "source": [
    "**Text preprocessing** \n",
    "\n",
    "There are a few types of preprocessing to improve how we model with words. The first is \"lemmatizing.\" The \"lemma\" of a word is its base form. For example, \"walk\" is the lemma of the word \"walking\". So, when you lemmatize the word walking, you would convert it to walk.\n",
    "\n",
    "It's also common to remove stopwords. Stopwords are words that occur frequently in the language and don't contain much information. English stopwords include <em>\"the\", \"is\", \"and\", \"but\", \"not\".</em>\n",
    "\n",
    "With a spaCy token, `token.lemma_` returns the lemma, while `token.is_stop` returns a boolean `True` if the token is a stopword (and `False` otherwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbaf87b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T12:38:21.479318Z",
     "start_time": "2021-07-20T12:38:21.470324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token \t\tLemma \t\tStopword\n",
      "----------------------------------------\n",
      "Tea\t\ttea\t\tFalse\n",
      "is\t\tbe\t\tTrue\n",
      "healthy\t\thealthy\t\tFalse\n",
      "and\t\tand\t\tTrue\n",
      "calming\t\tcalm\t\tFalse\n",
      ",\t\t,\t\tFalse\n",
      "do\t\tdo\t\tTrue\n",
      "n't\t\tn't\t\tTrue\n",
      "you\t\tyou\t\tTrue\n",
      "think\t\tthink\t\tFalse\n",
      "?\t\t?\t\tFalse\n"
     ]
    }
   ],
   "source": [
    "print(f\"Token \\t\\tLemma \\t\\tStopword\".format('Token', 'Lemma', 'Stopword'))\n",
    "print(\"-\"*40)\n",
    "for token in doc:\n",
    "    print(f\"{str(token)}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814ae009",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T12:40:06.232940Z",
     "start_time": "2021-07-20T12:40:06.228942Z"
    }
   },
   "source": [
    "Why are lemmas and identifying stopwords important? Language data has a lot of noise mixed in with informative content. In the sentence above, the important words are tea, healthy and calming. Removing stop words might help the predictive model focus on relevant words. Lemmatizing similarly helps by combining multiple forms of the same word into one base form (\"calming\", \"calms\", \"calmed\" would all change to \"calm\").\n",
    "\n",
    "However, lemmatizing and dropping stopwords might result in your models performing worse. So you should treat this preprocessing as part of your hyperparameter optimization process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2120dc",
   "metadata": {},
   "source": [
    "**Pattern Matching**\n",
    "\n",
    "Another common NLP task is matching tokens or phrases within chunks of text or whole documents. You can do pattern matching with regular expressions, but spaCy's matching capabilities tend to be easier to use.\n",
    "\n",
    "To match individual tokens, you create a Matcher. When you want to match a list of terms, it's easier and more efficient to use `PhraseMatcher`. For example, if you want to find where different smartphone models show up in some text, you can create patterns for the model names of interest. First you create the PhraseMatcher itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68b84c49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T12:45:12.374581Z",
     "start_time": "2021-07-20T12:45:12.369602Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab, attr='LOWER')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a7c9e4",
   "metadata": {},
   "source": [
    "The matcher is created using the vocabulary of your model. Here we're using the small English model you loaded earlier. Setting `attr='LOWER'` will match the phrases on lowercased text. This provides case insensitive matching.\n",
    "\n",
    "Next you create a list of terms to match in the text. The phrase matcher needs the patterns as document objects. The easiest way to get these is with a list comprehension using the nlp model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65277e62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T12:48:53.571402Z",
     "start_time": "2021-07-20T12:48:53.530383Z"
    }
   },
   "outputs": [],
   "source": [
    "terms = ['Galaxy Note', 'iPhone 11', 'iPhone XS', 'Google Pixel']\n",
    "patterns = [nlp(text) for text in terms]\n",
    "matcher.add(\"TerminologyList\", patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cac18b",
   "metadata": {},
   "source": [
    "Then you create a document from the text to search and use the phrase matcher to find where the terms occur in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e31d32da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T12:52:13.151193Z",
     "start_time": "2021-07-20T12:52:13.133201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3766102292120407359, 17, 19), (3766102292120407359, 22, 24), (3766102292120407359, 30, 32), (3766102292120407359, 33, 35)]\n"
     ]
    }
   ],
   "source": [
    "text_doc = nlp(\"Glowing review overall, and some really interesting side-by-side \"\n",
    "               \"photography tests pitting the iPhone 11 Pro against the \"\n",
    "               \"Galaxy Note 10 Plus and last yearâ€™s iPhone XS and Google Pixel 3.\") \n",
    "\n",
    "matches=matcher(text_doc)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0719ccd4",
   "metadata": {},
   "source": [
    "The matches here are a tuple of the match id and the positions of the start and end of the phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9a79761",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T15:10:02.135136Z",
     "start_time": "2021-07-20T15:10:02.129139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TerminologyList iPhone 11\n"
     ]
    }
   ],
   "source": [
    "match_id, start, end = matches[0]\n",
    "print(nlp.vocab.strings[match_id], text_doc[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d0b87c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T15:10:11.987622Z",
     "start_time": "2021-07-20T15:10:11.981642Z"
    }
   },
   "source": [
    "**Basic Text Processing with Spacy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56099e3b",
   "metadata": {},
   "source": [
    "You're a consultant for DelFalco's Italian Restaurant. The owner asked you to identify whether there are any foods on their menu that diners find disappointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9e3b2a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T15:12:02.927936Z",
     "start_time": "2021-07-20T15:11:55.538536Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8109a323",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T15:33:43.551859Z",
     "start_time": "2021-07-20T15:33:43.488896Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>lDJIaF4eYRF4F7g6Zb9euw</td>\n",
       "      <td>lb0QUR5bc4O-Am4hNq9ZGg</td>\n",
       "      <td>r5PLDU-4mSbde5XekTXSCA</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I used to work food service and my manager at ...</td>\n",
       "      <td>2013-01-27 17:54:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>vvIzf3pr8lTqE_AOsxmgaA</td>\n",
       "      <td>MAmijW4ooUzujkufYYLMeQ</td>\n",
       "      <td>r5PLDU-4mSbde5XekTXSCA</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>We have been trying Eggplant sandwiches all ov...</td>\n",
       "      <td>2015-04-15 04:50:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>UF-JqzMczZ8vvp_4tPK3bQ</td>\n",
       "      <td>slfi6gf_qEYTXy90Sw93sg</td>\n",
       "      <td>r5PLDU-4mSbde5XekTXSCA</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Amazing Steak and Cheese... Better than any Ph...</td>\n",
       "      <td>2011-03-20 00:57:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>geUJGrKhXynxDC2uvERsLw</td>\n",
       "      <td>N_-UepOzAsuDQwOUtfRFGw</td>\n",
       "      <td>r5PLDU-4mSbde5XekTXSCA</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Although I have been going to DeFalco's for ye...</td>\n",
       "      <td>2018-07-17 01:48:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1354</th>\n",
       "      <td>aPctXPeZW3kDq36TRm-CqA</td>\n",
       "      <td>139hD7gkZVzSvSzDPwhNNw</td>\n",
       "      <td>r5PLDU-4mSbde5XekTXSCA</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Highs: Ambience, value, pizza and deserts. Thi...</td>\n",
       "      <td>2018-01-21 10:52:58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   review_id                 user_id             business_id  \\\n",
       "109   lDJIaF4eYRF4F7g6Zb9euw  lb0QUR5bc4O-Am4hNq9ZGg  r5PLDU-4mSbde5XekTXSCA   \n",
       "1013  vvIzf3pr8lTqE_AOsxmgaA  MAmijW4ooUzujkufYYLMeQ  r5PLDU-4mSbde5XekTXSCA   \n",
       "1204  UF-JqzMczZ8vvp_4tPK3bQ  slfi6gf_qEYTXy90Sw93sg  r5PLDU-4mSbde5XekTXSCA   \n",
       "1251  geUJGrKhXynxDC2uvERsLw  N_-UepOzAsuDQwOUtfRFGw  r5PLDU-4mSbde5XekTXSCA   \n",
       "1354  aPctXPeZW3kDq36TRm-CqA  139hD7gkZVzSvSzDPwhNNw  r5PLDU-4mSbde5XekTXSCA   \n",
       "\n",
       "      stars  useful  funny  cool  \\\n",
       "109       4       2      0     0   \n",
       "1013      4       0      0     0   \n",
       "1204      5       1      0     0   \n",
       "1251      1       0      0     0   \n",
       "1354      2       0      0     0   \n",
       "\n",
       "                                                   text                date  \n",
       "109   I used to work food service and my manager at ... 2013-01-27 17:54:54  \n",
       "1013  We have been trying Eggplant sandwiches all ov... 2015-04-15 04:50:56  \n",
       "1204  Amazing Steak and Cheese... Better than any Ph... 2011-03-20 00:57:45  \n",
       "1251  Although I have been going to DeFalco's for ye... 2018-07-17 01:48:23  \n",
       "1354  Highs: Ambience, value, pizza and deserts. Thi... 2018-01-21 10:52:58  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_json('data/restaurant.json')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ad0173b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T15:33:30.450695Z",
     "start_time": "2021-07-20T15:33:30.444701Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11889"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b653f48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T15:16:44.115632Z",
     "start_time": "2021-07-20T15:16:44.108637Z"
    }
   },
   "outputs": [],
   "source": [
    "menu = [\"Cheese Steak\", \"Cheesesteak\", \"Steak and Cheese\", \"Italian Combo\", \"Tiramisu\", \"Cannoli\",\n",
    "        \"Chicken Salad\", \"Chicken Spinach Salad\", \"Meatball\", \"Pizza\", \"Pizzas\", \"Spaghetti\",\n",
    "        \"Bruchetta\", \"Eggplant\", \"Italian Beef\", \"Purista\", \"Pasta\", \"Calzones\",  \"Calzone\",\n",
    "        \"Italian Sausage\", \"Chicken Cutlet\", \"Chicken Parm\", \"Chicken Parmesan\", \"Gnocchi\",\n",
    "        \"Chicken Pesto\", \"Turkey Sandwich\", \"Turkey Breast\", \"Ziti\", \"Portobello\", \"Reuben\",\n",
    "        \"Mozzarella Caprese\",  \"Corned Beef\", \"Garlic Bread\", \"Pastrami\", \"Roast Beef\",\n",
    "        \"Tuna Salad\", \"Lasagna\", \"Artichoke Salad\", \"Fettuccini Alfredo\", \"Chicken Parmigiana\",\n",
    "        \"Grilled Veggie\", \"Grilled Veggies\", \"Grilled Vegetable\", \"Mac and Cheese\", \"Macaroni\",  \n",
    "         \"Prosciutto\", \"Salami\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548c7aed",
   "metadata": {},
   "source": [
    "The owner also gave you this list of menu items and common alternate spellings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a8ea25",
   "metadata": {},
   "source": [
    "**Step 2: Find items in one review**\n",
    "\n",
    "You'll pursue this plan of calculating average scores of the reviews mentioning each menu item.\n",
    "\n",
    "As a first step, you'll write code to extract the foods mentioned in a single review.\n",
    "\n",
    "Since menu items are multiple tokens long, you'll use `PhraseMatcher` which can match series of tokens.\n",
    "\n",
    "Fill in the `____` values below to get a list of items matching a single menu item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37d8b458",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T16:48:19.367627Z",
     "start_time": "2021-07-20T16:48:19.152755Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "index_of_review_to_test_on=14\n",
    "text_to_test_on = data.text.iloc[index_of_review_to_test_on]\n",
    "\n",
    "# Load the SpaCy model\n",
    "nlp=spacy.blank('en')\n",
    "\n",
    "#Create the tokenized version of text_to_test_on\n",
    "review_doc= nlp(text_to_test_on)\n",
    "\n",
    "# Create the PhraseMatcher object. The tokenizer is the first argument. Use attr = 'LOWER' to make consistent capitalization\n",
    "matcher=PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "\n",
    "# Create a list of tokens for each item in the menu\n",
    "\n",
    "menu_tokens_list=[nlp(item) for item in menu]\n",
    "\n",
    "# Add the item patterns to the matcher. \n",
    "# Look at https://spacy.io/api/phrasematcher#add in the docs for help with this step\n",
    "# Then uncomment the lines below \n",
    "\n",
    "matcher.add(\"MENU\",menu_tokens_list)\n",
    "\n",
    "# Find matches in the review_doc\n",
    "matches= matcher(review_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c03c7a71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T16:52:17.751082Z",
     "start_time": "2021-07-20T16:52:17.746083Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token number 58: meatball\n",
      "Token number 58: meatball\n",
      "Token number 58: meatball\n"
     ]
    }
   ],
   "source": [
    "for matches in matches:\n",
    "    print(f\"Token number {match[1]}: {review_doc[match[1]:match[2]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c321f6",
   "metadata": {},
   "source": [
    "**Step 3: Matching on the whole dataset**\n",
    "\n",
    "Now run this matcher over the whole dataset and collect ratings for each menu item. Each review has a rating, `review.stars`. For each item that appears in the review text (`review.text`), append the review's rating to a list of ratings for that item. The lists are kept in a dictionary `item_ratings`.\n",
    "\n",
    "To get the matched phrases, you can reference the `PhraseMatcher` documentation for the structure of each match object:\n",
    "\n",
    ">A list of `(match_id, start, end)` tuples, describing the matches. A match tuple describes a span `doc[start:end]`. The `match_id` is the ID of the added match pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bbd1a0a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T17:34:40.709381Z",
     "start_time": "2021-07-20T17:34:39.583790Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# item_ratings is a dictionary of lists. If a key doesn't exist in item_ratings,\n",
    "# the key is added with an empty list as the value.\n",
    "item_rating=defaultdict(list)\n",
    "\n",
    "for idx, review in data.iterrows():\n",
    "    doc=nlp(review.text)\n",
    "    # Using the matcher from the previous exercise\n",
    "    matches=matcher(doc)\n",
    "    # Create a set of the items found in the review text\n",
    "    found_items = set([doc[match[1]:match[2]] for match in matches])\n",
    "    \n",
    "    # Update item_ratings with rating for each item in found_items\n",
    "    # Transform the item strings to lowercase to make it case insensitive\n",
    "    \n",
    "    for item in found_items:\n",
    "        item_rating[item].append(review.stars)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e53374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean ratings for each menu item as a dictionary\n",
    "mean_ratings = {item:sum(ratings)/len(ratings) for item,ratings in item_ratings.items()}\n",
    "\n",
    "# Find the worst item, and write it as a string in worst_item. This can be multiple lines of code if you want.\n",
    "worst_item = sorted(mean_ratings, key=mean_ratings.get)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f7beb0",
   "metadata": {},
   "source": [
    "**Step 5: Are counts important here?**\n",
    "\n",
    "Similar to the mean ratings, you can calculate the number of reviews for each item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "44e573af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T17:36:58.143273Z",
     "start_time": "2021-07-20T17:36:58.138293Z"
    }
   },
   "outputs": [],
   "source": [
    "counts = {item: len(ratings) for item, ratings in item_ratings.items()}\n",
    "\n",
    "item_counts = sorted(counts, key=counts.get, reverse=True)\n",
    "for item in item_counts:\n",
    "    print(f\"{item:>25}{counts[item]:>5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f7d0ca",
   "metadata": {},
   "source": [
    "**Text Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa0d80b",
   "metadata": {},
   "source": [
    "Text Classification with SpaCy\n",
    "A common task in NLP is text classification. This is \"classification\" in the conventional machine learning sense, and it is applied to text. Examples include spam detection, sentiment analysis, and tagging customer queries.\n",
    "\n",
    "In this tutorial, you'll learn text classification with spaCy. The classifier will detect spam messages, a common functionality in most email clients. Here is an overview of the data you'll use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cf55b0a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T17:56:26.050346Z",
     "start_time": "2021-07-20T17:56:25.707997Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "5  spam  FreeMsg Hey there darling it's been 3 week's n...\n",
       "6   ham  Even my brother is not like to speak with me. ...\n",
       "7   ham  As per your request 'Melle Melle (Oru Minnamin...\n",
       "8  spam  WINNER!! As a valued network customer you have...\n",
       "9  spam  Had your mobile 11 months or more? U R entitle..."
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading the spam data\n",
    "# ham is the label for non-spam messages\n",
    "spam = pd.read_csv('data/spam.csv')\n",
    "spam.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9150e445",
   "metadata": {},
   "source": [
    "**Bag of Words**\n",
    "\n",
    "Machine learning models don't learn from raw text data. Instead, you need to convert the text to something numeric.\n",
    "\n",
    "The simplest common representation is a variation of one-hot encoding. You represent each document as a vector of term frequencies for each term in the vocabulary. The vocabulary is built from all the tokens (terms) in the corpus (the collection of documents).\n",
    "\n",
    "As an example, take the sentences \"Tea is life. Tea is love.\" and \"Tea is healthy, calming, and delicious.\" as our corpus. The vocabulary then is {\"tea\", \"is\", \"life\", \"love\", \"healthy\", \"calming\", \"and\", \"delicious\"} (ignoring punctuation).\n",
    "\n",
    "For each document, count up how many times a term occurs, and place that count in the appropriate element of a vector. The first sentence has \"tea\" twice and that is the first position in our vocabulary, so we put the number 2 in the first element of the vector. Our sentences as vectors then look like\n",
    "\n",
    "v1=[22110000]\n",
    "v2=[11001111]\n",
    " \n",
    "This is called the bag of words representation. You can see that documents with similar terms will have similar vectors. Vocabularies frequently have tens of thousands of terms, so these vectors can be very large.\n",
    "\n",
    "Another common representation is TF-IDF (Term Frequency - Inverse Document Frequency). TF-IDF is similar to bag of words except that each term count is scaled by the term's frequency in the corpus. Using TF-IDF can potentially improve your models. You won't need it here. Feel free to look it up though!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcefa0e3",
   "metadata": {},
   "source": [
    "**Building a Bag of Words model**\n",
    "\n",
    "Once you have your documents in a bag of words representation, you can use those vectors as input to any machine learning model. spaCy handles the bag of words conversion and building a simple linear model for you with the `TextCategorizer` class.\n",
    "\n",
    "The `TextCategorizer` is a spaCy pipe. Pipes are classes for processing and transforming tokens. When you create a spaCy model with `nlp = spacy.load('en_core_web_sm')`, there are default pipes that perform part of speech tagging, entity recognition, and other transformations. When you run text through a model `doc = nlp(\"Some text here\")`, the output of the pipes are attached to the tokens in the doc object. The lemmas for `token.lemma_` come from one of these pipes.\n",
    "\n",
    "You can remove or add pipes to models. What we'll do here is create an empty model without any pipes (other than a tokenizer, since all models always have a tokenizer). Then, we'll create a TextCategorizer pipe and add it to the empty model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a086ad8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T18:33:23.531577Z",
     "start_time": "2021-07-20T18:33:23.324534Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Create an empty model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Create the TextCategorizer with exclusive classes and \"bow\" architecture\n",
    "textcat = nlp.create_pipe(\n",
    "              \"textcat\",\n",
    "              config={\n",
    "                \"exclusive_classes\": True,\n",
    "                \"architecture\": \"bow\"})\n",
    "\n",
    "# Add the TextCategorizer to the empty model\n",
    "nlp.add_pipe(textcat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20344cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels to text classifier\n",
    "textcat.add_label(\"ham\")\n",
    "textcat.add_label(\"spam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68adcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Create an empty model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Create the TextCategorizer with exclusive classes and \"bow\" architecture\n",
    "textcat = nlp.create_pipe(\n",
    "              \"textcat\",\n",
    "              config={\n",
    "                \"exclusive_classes\": True,\n",
    "                \"architecture\": \"bow\"})\n",
    "\n",
    "# Add the TextCategorizer to the empty model\n",
    "nlp.add_pipe(textcat)\n",
    "\n",
    "# Add labels to text classifier\n",
    "textcat.add_label(\"ham\")\n",
    "textcat.add_label(\"spam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca04876",
   "metadata": {},
   "source": [
    "**Word Embeddings**\n",
    "\n",
    "You know at this point that machine learning on text requires that you first represent the text numerically. So far, you've done this with bag of words representations. But you can usually do better with word embeddings.\n",
    "\n",
    "Word embeddings (also called word vectors) represent each word numerically in such a way that the vector corresponds to how that word is used or what it means. Vector encodings are learned by considering the context in which the words appear. Words that appear in similar contexts will have similar vectors. For example, vectors for \"leopard\", \"lion\", and \"tiger\" will be close together, while they'll be far away from \"planet\" and \"castle\".\n",
    "\n",
    "Even cooler, relations between words can be examined with mathematical operations. Subtracting the vectors for \"man\" and \"woman\" will return another vector. If you add that to the vector for \"king\" the result is close to the vector for \"queen.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b12337a",
   "metadata": {},
   "source": [
    "These vectors can be used as features for machine learning models. Word vectors will typically improve the performance of your models above bag of words encoding. spaCy provides embeddings learned from a model called Word2Vec. You can access them by loading a large language model like `en_core_web_lg`. Then they will be available on tokens from the `.vector` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4fc42079",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T20:05:31.064352Z",
     "start_time": "2021-07-20T20:05:12.696173Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "# Need to load the large model to get the vectors\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f34a41ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T20:07:21.734630Z",
     "start_time": "2021-07-20T20:07:16.285628Z"
    }
   },
   "outputs": [],
   "source": [
    "# Disabling other pipes because we don't need them and it'll speed up this part a bit\n",
    "text = \"These vectors can be used as features for machine learning models.\"\n",
    "with nlp.disable_pipes():\n",
    "    vectors = np.array([token.vector for token in  nlp(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b7685ee5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T20:08:05.078316Z",
     "start_time": "2021-07-20T20:08:04.540970Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 300)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1e585d",
   "metadata": {},
   "source": [
    "These are 300-dimensional vectors, with one vector for each word. However, we only have document-level labels and our models won't be able to use the word-level embeddings. So, you need a vector representation for the entire document.\n",
    "\n",
    "There are many ways to combine all the word vectors into a single document vector we can use for model training. A simple and surprisingly effective approach is simply averaging the vectors for each word in the document. Then, you can use these document vectors for modeling.\n",
    "\n",
    "spaCy calculates the average document vector which you can get with `doc.vector`. Here is an example loading the spam data and converting it to document vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6e8ccf6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T20:11:41.267354Z",
     "start_time": "2021-07-20T20:10:04.993558Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 300)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading the spam data\n",
    "# ham is the label for non-spam messages\n",
    "spam = pd.read_csv('data/spam.csv')\n",
    "\n",
    "with nlp.disable_pipes():\n",
    "    doc_vectors = np.array([nlp(text).vector for text in spam.text])\n",
    "    \n",
    "doc_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949b0e67",
   "metadata": {},
   "source": [
    "**Classification Models**\n",
    "\n",
    "With the document vectors, you can train scikit-learn models, xgboost models, or any other standard approach to modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e034b4d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T20:16:27.122991Z",
     "start_time": "2021-07-20T20:15:14.834559Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(doc_vectors, spam.label,\n",
    "                                                    test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c878f028",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T20:17:17.949703Z",
     "start_time": "2021-07-20T20:17:17.944706Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5014, 300), (558, 300), (5014,), (558,))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ece39bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3514227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f7ab86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f30e93c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c758ee4",
   "metadata": {},
   "source": [
    "**Farrukh Bulbulov**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
